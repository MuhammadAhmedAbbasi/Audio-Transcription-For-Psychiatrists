{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install audioread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tempfile\n",
    "import librosa\n",
    "\n",
    "# URL of the audio file\n",
    "url = 'https://112.124.48.213:443/file/audio/124d5a90-081c-b7dd-9e47-778f0b1a091f.wav'\n",
    "\n",
    "# Download the audio file with SSL verification disabled\n",
    "response = requests.get(url, verify=False)\n",
    "\n",
    "# Create a temporary file and write the audio content to it\n",
    "with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:\n",
    "    temp_file.write(response.content)\n",
    "    temp_file_path = temp_file.name  # Get the path of the temporary file\n",
    "\n",
    "# Load the audio using librosa\n",
    "y, sr = librosa.load(temp_file_path, sr=None)\n",
    "\n",
    "# Now y contains the audio signal, and sr is the sample rate\n",
    "print(f\"Audio data shape: {y.shape}, Sample rate: {sr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from funasr import AutoModel\n",
    "from funasr.utils.postprocess_utils import rich_transcription_postprocess\n",
    "import librosa\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "from IPython.display import Audio as IPythonAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import sounddevice as sd\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "def load_audio(file_path):\n",
    "    # Load the audio file with the original sample rate\n",
    "    audio_data, sample_rate = librosa.load(file_path, sr=None)\n",
    "    return audio_data, sample_rate\n",
    "\n",
    "def play_audio(audio_data, sample_rate):\n",
    "    # Play the audio data\n",
    "    sd.play(audio_data, sample_rate)\n",
    "    sd.wait()  # Wait until the sound has finished playing\n",
    "\n",
    "def save_audio(audio_data, sample_rate, save_path):\n",
    "    # Save the audio data as a .wav file using scipy\n",
    "    # Convert the floating-point audio to 16-bit PCM\n",
    "    audio_data_int16 = (audio_data * 32767).astype('int16')\n",
    "    wav.write(save_path, sample_rate, audio_data_int16)\n",
    "    print(f\"Audio saved to {save_path}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = r'C:\\Users\\Administrator\\Desktop\\Backend-Algorithm-LLM\\Audio_transcription_files\\tmp_5_no7xo.wav'\n",
    "save_path = r'C:\\Users\\Administrator\\Desktop\\Backend-Algorithm-LLM\\Audio_transcription_files\\output_audio.wav'\n",
    "\n",
    "audio_data, sample_rate = load_audio(file_path)\n",
    "\n",
    "# Save the audio to a new .wav file\n",
    "save_audio(audio_data, sample_rate, save_path)\n",
    "\n",
    "# Play the audio from the saved file\n",
    "play_audio(audio_data, sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:\\Users\\Administrator\\Desktop\\Backend-Algorithm-LLM\\Audio_transcription_files\\h.wav'\n",
    "audio_data, sample_rate = librosa.load(file_path, sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "# Load the file\n",
    "file_path = r'C:\\Users\\Administrator\\Desktop\\Backend-Algorithm-LLM\\Audio_transcription_files\\output_audio.wav'\n",
    "\n",
    "# Open the audio file to get the details\n",
    "with sf.SoundFile(file_path) as file:\n",
    "    # Bit depth is part of the file's format attributes\n",
    "    bit_depth = file.subtype  # This gives the bit depth and encoding format\n",
    "    sample_rate = file.samplerate\n",
    "    channels = file.channels\n",
    "\n",
    "    print(f\"Bit Depth: {bit_depth}\")\n",
    "    print(f\"Sample Rate: {sample_rate}\")\n",
    "    print(f\"Channels: {channels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of channels\n",
    "if audio_data.ndim == 1:\n",
    "    print(\"Mono audio (1 channel)\")\n",
    "elif audio_data.ndim == 2:\n",
    "    num_channels = audio_data.shape[0]\n",
    "    print(f\"Stereo or multi-channel audio with {num_channels} channels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file_path):\n",
    "    audio_data, sample_rate = librosa.load(file_path, sr=None)\n",
    "    return audio_data, sample_rate\n",
    "\n",
    "def play_audio(audio_data, sample_rate):\n",
    "    sd.play(audio_data, sample_rate)\n",
    "    sd.wait()  # Wait until the sound has finished playing\n",
    "\n",
    "# Example usage\n",
    "file_path = r'C:\\Users\\Administrator\\Desktop\\Backend-Algorithm-LLM\\Audio_transcription_files\\test\\tmp1gjsg3lv.mp3'\n",
    "audio_data, sample_rate = load_audio(file_path)\n",
    "play_audio(audio_data, sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details\n",
    "print(\"Sample Rate:\", sample_rate)\n",
    "print(\"Audio Array:\", audio_array)\n",
    "print(\"Audio Array datatype is Array: \", isinstance(audio_array,np.ndarray) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling the Audio so it matches to the requirement of the model\n",
    "audio_16KHz = librosa.resample(audio_array,\n",
    "                               orig_sr=sample_rate,\n",
    "                               target_sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"FunAudioLLM/SenseVoiceSmall\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SenseVoiceSmall \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dir = r'C:/Users/Administrator/Desktop/LLM_work/SenseVoiceSmall'\n",
    "# pretrained_model_path=  r'C:\\Users\\Administrator\\Desktop\\LLM_work\\SenseVoiceSmall\\model.pt'\n",
    "model = AutoModel(\n",
    "    model='iic/SenseVoiceSmall',\n",
    "    # init_param = pretrained_model_path\n",
    ")\n",
    "\n",
    "# en\n",
    "res = model.generate(\n",
    "    input=audio_16KHz,\n",
    "    cache={},\n",
    "    language=\"auto\",  # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n",
    "    use_itn=True,\n",
    "    batch_size_s=60,\n",
    "    merge_vad=True,  #\n",
    "    merge_length_s=15,\n",
    ")\n",
    "text = rich_transcription_postprocess(res[0][\"text\"])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# paraformer-zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = r'C:\\Users\\Administrator\\Desktop\\Backend-Algorithm-LLM\\Audio_transcription_files\\h.wav'\n",
    "audio_array, sample_rate = librosa.load(audio_path, sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel(model='paraformer-zh',kwargs={\"disable_update\": True, \"device\": \"cpu\"},punc_model =\"ct-punc\", vad_model=\"fsmn-vad\", \n",
    "                                                     vad_kwargs={\"max_single_segment_time\": 30000},spk_model=\"cam++\", \n",
    "                                                     spk_model_revision=\"v2.0.2\")\n",
    "\n",
    "\n",
    "# en\n",
    "res = model.generate(\n",
    "    input=audio_16KHz,\n",
    "    cache={},\n",
    "    language=\"zn\",\n",
    "    use_itn=True,\n",
    "    batch_size_s=60,\n",
    "    merge_vad=True,  #\n",
    "    merge_length_s=15,\n",
    ")\n",
    "text = rich_transcription_postprocess(res[0][\"text\"])\n",
    "print(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ct-punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is used to detect the punctuation in the generated text\n",
    "model = AutoModel(model=\"ct-punc\", model_revision=\"v2.0.4\")\n",
    "res = model.generate(input=\"那今天的会就到这里吧 happy new year 明年见\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fsmn-vad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funasr import AutoModel\n",
    "\n",
    "model = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\")\n",
    "\n",
    "res = model.generate(input=audio_16KHz)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel(\n",
    "    model='iic/SenseVoiceSmall',\n",
    "     vad_model=\"fsmn-vad\",\n",
    "     vad_kwargs={\"max_single_segment_time\": 30000},\n",
    "    device=\"cpu\",\n",
    "    #punc_model = \"ct-punc\"\n",
    ")\n",
    "\n",
    "# en\n",
    "res = model.generate(\n",
    "    input=audio_16KHz,\n",
    "    cache={},\n",
    "    language=\"zn\",  # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n",
    "    use_itn=True,\n",
    "    batch_size_s=60,\n",
    "    merge_vad=True,  #\n",
    "    merge_length_s=15,\n",
    ")\n",
    "text = rich_transcription_postprocess(res[0][\"text\"])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transcription to a text file\n",
    "with open(\"transcription_SenseVoiceSmall-fsmn-vad.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intensity Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import logging\n",
    "\n",
    "def text_generator(model, audio_sample):\n",
    "    text = model.generate(\n",
    "        input=audio_sample,\n",
    "        cache={},\n",
    "        language=\"auto\",\n",
    "        use_itn=True,\n",
    "        batch_size_s=60,\n",
    "        merge_vad=False\n",
    "    )\n",
    "    return text\n",
    "    \n",
    "def compute_intensity(segment):\n",
    "    \"\"\"Compute the intensity of a given audio segment as RMS.\"\"\"\n",
    "    return np.sqrt(np.mean(segment**2))\n",
    "\n",
    "def filter_high_intensity_segments(audio_array, segment_duration: float, intensity_threshold: float):\n",
    "    \"\"\"Filter high-intensity audio segments from the given audio array.\"\"\"\n",
    "    segment_samples = int(segment_duration * 16000)  # Number of samples per segment\n",
    "    high_intensity_segments = []\n",
    "\n",
    "    # Split audio into segments and process their intensities\n",
    "    num_segments = len(audio_array) // segment_samples\n",
    "    for i in range(num_segments):\n",
    "        start_sample = i * segment_samples\n",
    "        end_sample = start_sample + segment_samples\n",
    "        segment = audio_array[start_sample:end_sample]\n",
    "        \n",
    "        # Calculate intensity for each segment\n",
    "        intensity = compute_intensity(segment)\n",
    "        print(f\"Segment {i+1} Intensity: {intensity:.6f}\")\n",
    "        \n",
    "        # Only keep the segment if its intensity exceeds the threshold\n",
    "        if intensity > intensity_threshold:\n",
    "            logging.info(f\"Keeping high-intensity segment {i+1} with intensity {intensity:.6f}\")\n",
    "            high_intensity_segments.append(segment)\n",
    "        else:\n",
    "            logging.info(f\"Skipping low-intensity segment {i+1} with intensity {intensity:.6f}\")\n",
    "\n",
    "    # If there are remaining samples that don't form a full segment, calculate their intensity\n",
    "    if len(audio_array) % segment_samples != 0:\n",
    "        segment = audio_array[num_segments * segment_samples:]\n",
    "        intensity = compute_intensity(segment)\n",
    "        print(f\"Last Segment Intensity: {intensity:.6f}\")\n",
    "        if intensity > intensity_threshold:\n",
    "            logging.info(f\"Keeping last high-intensity segment with intensity {intensity:.6f}\")\n",
    "            high_intensity_segments.append(segment)\n",
    "\n",
    "    return high_intensity_segments\n",
    "\n",
    "def audio_to_text_model_offline(audio_file_path: str, model, intensity_threshold: float = 0.00) -> str:\n",
    "    \"\"\"Converts audio file to text using the provided model, processing only high-intensity segments.\"\"\"\n",
    "    try:\n",
    "        # Load the audio file\n",
    "        audio_array, sampling_rate = librosa.load(audio_file_path, sr=None)\n",
    "        audio_array = librosa.resample(audio_array, orig_sr=sampling_rate, target_sr=16000)\n",
    "\n",
    "        # Define the duration of each segment (for example, 2 seconds)\n",
    "        segment_duration = 2.0  # seconds\n",
    "\n",
    "        # Filter high-intensity segments\n",
    "        high_intensity_segments = filter_high_intensity_segments(audio_array, segment_duration, intensity_threshold)\n",
    "\n",
    "        # Concatenate high-intensity segments into a single array\n",
    "        if high_intensity_segments:\n",
    "            combined_audio = np.concatenate(high_intensity_segments)\n",
    "            logging.info(\"Transcribing combined audio of length: %d\", len(combined_audio))\n",
    "            print(combined_audio)\n",
    "\n",
    "            # Check if the length of the combined audio is greater than 30 seconds\n",
    "            if len(combined_audio) > (30 * 16000):\n",
    "                transcriptions = []\n",
    "                sample_length = int(30 * 16000)\n",
    "                for start in range(0, len(combined_audio), sample_length):\n",
    "                    end = min(start + sample_length, len(combined_audio))\n",
    "                    chunk = combined_audio[start:end]\n",
    "                    generated_text = text_generator(model, chunk)\n",
    "                    transcriptions.append(generated_text[0][\"text\"])\n",
    "                \n",
    "                # Join all transcriptions\n",
    "                transcription_result = \" \".join(transcriptions)\n",
    "            else:\n",
    "                # Transcribe the combined audio\n",
    "                res = model.generate(\n",
    "                    input=combined_audio,\n",
    "                    vad_model=\"fsmn-vad\",\n",
    "                    cache={},\n",
    "                    language=\"auto\",\n",
    "                    use_itn=True,\n",
    "                    batch_size_s=60,\n",
    "                    merge_vad=False\n",
    "                )\n",
    "                transcription_result = generated_text[0][\"text\"]\n",
    "        else:\n",
    "            logging.info(\"No high-intensity segments to transcribe.\")\n",
    "            transcription_result = \"\"\n",
    "\n",
    "        return transcription_result.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in audio_to_text_model_offline: %s\", e)\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funasr import AutoModel\n",
    "path = r'C:\\Users\\Administrator\\Desktop\\Backend-Algorithm-LLM\\Algorithm\\audio_text_llm\\SenseVoiceSmall\\audio_sample\\How to Learn Chinese Faster and Smarter.mp3'\n",
    "\n",
    "model = AutoModel(\n",
    "    model='paraformer-zh',  # This is the directory of model path\n",
    "    device=\"cuda:0\",  # \"cuda:0\" for GPU (if CUDA is available) or \"cpu\" for CPU.\n",
    "    hub=\"hf\",   # \"hf\" for Hugging Face Hub, \"local\" for local filesystem.\n",
    ")\n",
    "\n",
    "\n",
    "transcription_result = audio_to_text_model_offline(path, model)\n",
    "print(transcription_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "\n",
    "# Get the default input device details\n",
    "device_info = sd.query_devices(sd.default.device[0], 'input')\n",
    "\n",
    "# Print the sample rate of the input device\n",
    "print(f\"Device Sampling Rate: {device_info['default_samplerate']} Hz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "\n",
    "# Set the sampling rate\n",
    "sampling_rate = 44100  # Sample rate in Hz\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    \"\"\"Callback function to process the audio data.\"\"\"\n",
    "    if status:\n",
    "        print(status)\n",
    "    # Convert the input data to a 1D array\n",
    "    audio_data = indata[:, 0]  # Assuming mono audio\n",
    "    # Calculate intensity as the root mean square (RMS) of the audio samples\n",
    "    intensity = np.sqrt(np.mean(audio_data**2))  # RMS calculation\n",
    "    print(f'Intensity: {intensity*1000:.6f}')\n",
    "\n",
    "# Start recording from the microphone\n",
    "with sd.InputStream(callback=audio_callback, channels=1, samplerate=sampling_rate):\n",
    "    print(\"Recording... Press Ctrl+C to stop.\")\n",
    "    try:\n",
    "        # Keep the program running indefinitely\n",
    "        while True:\n",
    "            pass\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopped recording.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# Function to filter the audio based on intensity (e.g., removing low-intensity parts)\n",
    "def filter_audio_by_intensity(audio_data, threshold, frame_length=2048, hop_length=512):\n",
    "    \"\"\"\n",
    "    Filter audio based on RMS intensity for each frame using librosa's split method.\n",
    "    \n",
    "    Parameters:\n",
    "        audio_data: The audio data array (1D).\n",
    "        threshold: The intensity threshold to keep audio (scaled to original RMS).\n",
    "        frame_length: The number of samples in each frame (default 2048 for efficiency).\n",
    "        hop_length: The number of samples to shift between consecutive frames.\n",
    "    \"\"\"\n",
    "    # Use librosa to split the audio based on energy threshold (this works similarly to RMS-based intensity filtering)\n",
    "    # This split will return non-silent segments as start and end indices\n",
    "    intervals = librosa.effects.split(audio_data, top_db=threshold)\n",
    "\n",
    "    filtered_audio = []\n",
    "    \n",
    "    # Iterate over the intervals (start and end) and append the corresponding segments to filtered_audio\n",
    "    for start, end in intervals:\n",
    "        filtered_audio.append(audio_data[start:end])\n",
    "    \n",
    "    # Concatenate all non-silent segments into one array\n",
    "    filtered_audio = np.concatenate(filtered_audio)\n",
    "    \n",
    "    return filtered_audio\n",
    "\n",
    "# Load, filter based on intensity, and save the audio\n",
    "def process_audio(file_path, intensity_threshold=60, output_file=\"output_filtered.wav\"):\n",
    "    # Load the audio file using librosa (it automatically resamples to 22050 Hz by default)\n",
    "    audio_data, sampling_rate = librosa.load(file_path, sr=None)  # sr=None keeps the original sampling rate\n",
    "    print(f\"Original Sampling Rate: {sampling_rate} Hz\")\n",
    "    \n",
    "    # Filter the audio based on intensity threshold (using split to detect non-silent segments)\n",
    "    filtered_audio = filter_audio_by_intensity(audio_data, intensity_threshold)\n",
    "    \n",
    "    # Check if any audio remains after filtering\n",
    "    if filtered_audio.size > 0:\n",
    "        # Save the filtered audio to a new file using soundfile\n",
    "        sf.write(output_file, filtered_audio, sampling_rate)\n",
    "        print(f\"Filtered audio saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"No audio passed the intensity threshold.\")\n",
    "\n",
    "# Example usage\n",
    "file_path = r'C:\\Users\\Administrator\\Desktop\\Backend-Algorithm-LLM\\Algorithm\\audio_text_llm\\SenseVoiceSmall\\output_audio.wav'  # Replace with the path to your audio file\n",
    "intensity_threshold = 30  # Threshold for filtering (in terms of RMS, for split method it's in dB)\n",
    "process_audio(file_path, intensity_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "# Function to record 1 minute of audio\n",
    "def record_audio(duration=60, output_file='output_audio.wav', sample_rate=44100, channels=1):\n",
    "    \"\"\"\n",
    "    Record audio for a given duration and save it as a WAV file.\n",
    "    \n",
    "    :param duration: The duration of the recording in seconds (default is 60 seconds).\n",
    "    :param output_file: The name of the file to save the audio.\n",
    "    :param sample_rate: The sample rate of the recording (default is 44100).\n",
    "    :param channels: The number of audio channels (default is 1 for mono).\n",
    "    \"\"\"\n",
    "    print(f\"Recording for {duration} seconds...\")\n",
    "\n",
    "    # Record audio using sounddevice\n",
    "    audio_data = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=channels, dtype='int16')\n",
    "    sd.wait()  # Wait until recording is finished\n",
    "    \n",
    "    # Save the recorded audio to a file using SoundFile\n",
    "    sf.write(output_file, audio_data, sample_rate)\n",
    "    print(f\"Audio saved to {output_file}\")\n",
    "\n",
    "# Call the function to record for 1 minute (60 seconds)\n",
    "record_audio(duration=60, output_file='output_audio.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install noisereduce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import noisereduce as nr\n",
    "\n",
    "# Load the audio file\n",
    "audio_data, sampling_rate = librosa.load(r'C:\\Users\\Administrator\\Desktop\\Backend-Algorithm-LLM\\Algorithm\\audio_text_llm\\SenseVoiceSmall\\output_audio.wav', sr=None)\n",
    "\n",
    "# Apply noise reduction\n",
    "reduced_noise_audio = nr.reduce_noise(\n",
    "    y=audio_data,\n",
    "    sr=44100,\n",
    "    prop_decrease=1,  # Reduce noise by 50%\n",
    "    thresh_n_mult_nonstationary=3,  # Moderate noise threshold for non-stationary noise\n",
    "    n_std_thresh_stationary=50,  # Higher threshold for stationary noise\n",
    "    time_constant_s=50,  # Longer adaptation for smoother noise reduction\n",
    "    clip_noise_stationary=True,  # Clip stationary noise\n",
    "    use_torch=False,  # Use CPU-based method (if True, would use GPU if available)\n",
    "    device=\"cuda\",  # Only relevant if use_torch=True\n",
    ")\n",
    "\n",
    "# Save the cleaned audio\n",
    "sf.write('cleaned_audio.wav', reduced_noise_audio, sampling_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sampling Rate: 44100 Hz\n",
      "Original Intensity (RMS): 4.663745\n",
      "Frame 1 RMS: 0.925991\n",
      "Frame 2 RMS: 1.733966\n",
      "Frame 3 RMS: 0.939083\n",
      "Frame 4 RMS: 0.354866\n",
      "Frame 5 RMS: 8.948013\n",
      "Frame 6 RMS: 11.367237\n",
      "Frame 7 RMS: 11.611264\n",
      "Frame 8 RMS: 10.260269\n",
      "Frame 9 RMS: 10.627591\n",
      "Frame 10 RMS: 5.603942\n",
      "Frame 11 RMS: 3.864626\n",
      "Frame 12 RMS: 4.497665\n",
      "Frame 13 RMS: 0.619096\n",
      "Frame 14 RMS: 1.984582\n",
      "Frame 15 RMS: 3.468934\n",
      "Frame 16 RMS: 4.607754\n",
      "Frame 17 RMS: 0.176029\n",
      "Frame 18 RMS: 8.023485\n",
      "Frame 19 RMS: 8.450524\n",
      "Frame 20 RMS: 2.252986\n",
      "Frame 21 RMS: 2.176846\n",
      "Frame 22 RMS: 5.675308\n",
      "Frame 23 RMS: 3.541254\n",
      "Frame 24 RMS: 5.636208\n",
      "Frame 25 RMS: 3.040562\n",
      "Frame 26 RMS: 0.511544\n",
      "Frame 27 RMS: 0.543065\n",
      "Frame 28 RMS: 5.096907\n",
      "Frame 29 RMS: 5.273408\n",
      "Frame 30 RMS: 7.054164\n",
      "Frame 31 RMS: 0.352844\n",
      "Frame 32 RMS: 5.921265\n",
      "Frame 33 RMS: 2.128933\n",
      "Frame 34 RMS: 3.863606\n",
      "Frame 35 RMS: 21.350397\n",
      "Frame 36 RMS: 11.587578\n",
      "Frame 37 RMS: 10.729396\n",
      "Frame 38 RMS: 6.435291\n",
      "Frame 39 RMS: 0.205064\n",
      "Frame 40 RMS: 0.179457\n",
      "Frame 41 RMS: 2.636585\n",
      "Frame 42 RMS: 9.113693\n",
      "Frame 43 RMS: 2.440051\n",
      "Frame 44 RMS: 0.169622\n",
      "Frame 45 RMS: 0.255930\n",
      "Frame 46 RMS: 0.628405\n",
      "Frame 47 RMS: 8.338659\n",
      "Frame 48 RMS: 20.982856\n",
      "Frame 49 RMS: 14.411719\n",
      "Frame 50 RMS: 0.357057\n",
      "Frame 51 RMS: 2.503140\n",
      "Frame 52 RMS: 17.033451\n",
      "Frame 53 RMS: 15.239250\n",
      "Frame 54 RMS: 5.413894\n",
      "Frame 55 RMS: 1.887525\n",
      "Frame 56 RMS: 2.702930\n",
      "Frame 57 RMS: 0.411322\n",
      "Frame 58 RMS: 2.178652\n",
      "Frame 59 RMS: 1.330229\n",
      "Frame 60 RMS: 0.361094\n",
      "Frame 61 RMS: 0.433085\n",
      "Frame 62 RMS: 0.462371\n",
      "Frame 63 RMS: 2.924362\n",
      "Frame 64 RMS: 1.082613\n",
      "Frame 65 RMS: 0.747405\n",
      "Frame 66 RMS: 4.145138\n",
      "Frame 67 RMS: 4.836752\n",
      "Frame 68 RMS: 2.192006\n",
      "Frame 69 RMS: 3.988408\n",
      "Frame 70 RMS: 4.286174\n",
      "Frame 71 RMS: 0.918197\n",
      "Frame 72 RMS: 0.582498\n",
      "Frame 73 RMS: 0.295504\n",
      "Frame 74 RMS: 0.449702\n",
      "Frame 75 RMS: 0.148839\n",
      "Frame 76 RMS: 1.206491\n",
      "Frame 77 RMS: 0.327222\n",
      "Frame 78 RMS: 1.574657\n",
      "Frame 79 RMS: 0.278705\n",
      "Frame 80 RMS: 0.391606\n",
      "Frame 81 RMS: 0.495073\n",
      "Frame 82 RMS: 0.200139\n",
      "Frame 83 RMS: 0.802802\n",
      "Frame 84 RMS: 10.598556\n",
      "Frame 85 RMS: 18.134072\n",
      "Frame 86 RMS: 8.603743\n",
      "Frame 87 RMS: 11.969281\n",
      "Frame 88 RMS: 1.368055\n",
      "Frame 89 RMS: 1.254815\n",
      "Frame 90 RMS: 9.889828\n",
      "Frame 91 RMS: 10.018874\n",
      "Frame 92 RMS: 4.852576\n",
      "Frame 93 RMS: 3.721585\n",
      "Frame 94 RMS: 7.179251\n",
      "Frame 95 RMS: 12.444699\n",
      "Frame 96 RMS: 10.079616\n",
      "Frame 97 RMS: 13.320015\n",
      "Frame 98 RMS: 3.576529\n",
      "Frame 99 RMS: 9.788171\n",
      "Frame 100 RMS: 0.338485\n",
      "Frame 101 RMS: 0.155410\n",
      "Frame 102 RMS: 2.003608\n",
      "Frame 103 RMS: 4.023486\n",
      "Frame 104 RMS: 0.716702\n",
      "Frame 105 RMS: 1.674765\n",
      "Frame 106 RMS: 0.154776\n",
      "Frame 107 RMS: 0.150099\n",
      "Frame 108 RMS: 7.991376\n",
      "Frame 109 RMS: 7.673390\n",
      "Frame 110 RMS: 10.447424\n",
      "Frame 111 RMS: 6.613764\n",
      "Frame 112 RMS: 6.711376\n",
      "Frame 113 RMS: 5.664154\n",
      "Frame 114 RMS: 3.606980\n",
      "Frame 115 RMS: 0.257444\n",
      "Frame 116 RMS: 0.205008\n",
      "Frame 117 RMS: 3.822293\n",
      "Frame 118 RMS: 3.335501\n",
      "Frame 119 RMS: 7.101633\n",
      "Frame 120 RMS: 7.415277\n",
      "Filtered audio saved to output_filtered.wav\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "# Function to calculate the RMS intensity of the audio\n",
    "def calculate_intensity(audio_data):\n",
    "    # Calculate intensity as the root mean square (RMS) for the entire audio\n",
    "    return np.mean(np.abs(audio_data))\n",
    "\n",
    "# Function to manually split the audio into frames\n",
    "def split_audio_into_frames(audio_data, frame_length, hop_length):\n",
    "    frames = []\n",
    "    for start in range(0, len(audio_data) - frame_length + 1, hop_length):\n",
    "        frame = audio_data[start:start + frame_length]\n",
    "        frames.append(frame)\n",
    "    return frames\n",
    "\n",
    "# Function to filter the audio based on intensity (e.g., removing low-intensity parts)\n",
    "def filter_audio_by_intensity(audio_data, threshold, frame_length=22050, hop_length=22050):\n",
    "    \"\"\"\n",
    "    Filter audio based on RMS intensity for each frame.\n",
    "    \n",
    "    Parameters:\n",
    "        audio_data: The audio data array (1D).\n",
    "        threshold: The intensity threshold to keep audio (scaled to original RMS).\n",
    "        frame_length: The number of samples in each frame (default 1 second if sr=22050).\n",
    "        hop_length: The number of samples to shift between consecutive frames.\n",
    "    \"\"\"\n",
    "    # Split the audio data into frames manually (no overlap)\n",
    "    frames = split_audio_into_frames(audio_data, frame_length, hop_length)\n",
    "    \n",
    "    # Initialize a list to hold the filtered frames\n",
    "    filtered_audio = []\n",
    "\n",
    "    # Iterate over each frame and calculate its RMS\n",
    "    for i, frame in enumerate(frames):\n",
    "        rms = np.mean(np.abs(frame))\n",
    "\n",
    "        print(f\"Frame {i+1} RMS: {rms*1000:.6f}\")  # Print the RMS value of each frame\n",
    "        \n",
    "        if rms > threshold:  # Only keep frames above the threshold\n",
    "            filtered_audio.extend(frame)\n",
    "    \n",
    "    # Convert the filtered list back into a numpy array\n",
    "    return np.array(filtered_audio)\n",
    "\n",
    "# Load, filter based on intensity, and save the audio\n",
    "def process_audio(file_path, intensity_threshold=0.05, output_file=\"output_filtered.wav\"):\n",
    "    # Load the audio file using soundfile (to keep the original sampling rate)\n",
    "    audio_data, sampling_rate = sf.read(file_path)  # sf.read automatically detects the sampling rate\n",
    "    print(f\"Original Sampling Rate: {sampling_rate} Hz\")\n",
    "    \n",
    "    # Calculate and print the original intensity (RMS)\n",
    "    original_intensity = calculate_intensity(audio_data)\n",
    "    print(f\"Original Intensity (RMS): {original_intensity*1000:.6f}\")\n",
    "    \n",
    "    # Adjust the threshold by dividing it by 1000 to match the unamplified scale\n",
    "    intensity_threshold = intensity_threshold / 1000  # Adjust threshold based on your scaling\n",
    "    \n",
    "    # Filter the audio based on intensity threshold\n",
    "    filtered_audio = filter_audio_by_intensity(audio_data, intensity_threshold)\n",
    "    \n",
    "    # Check if any audio remains after filtering\n",
    "    if filtered_audio.size > 0:\n",
    "        # Save the filtered audio to a new file using soundfile\n",
    "        sf.write(output_file, filtered_audio, sampling_rate)\n",
    "        print(f\"Filtered audio saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"No audio passed the intensity threshold.\")\n",
    "\n",
    "# Example usage\n",
    "file_path = r'C:\\Users\\Administrator\\Desktop\\Backend-Algorithm-LLM\\Algorithm\\audio_text_llm\\SenseVoiceSmall\\output_audio.wav'  # Replace with the path to your audio file\n",
    "intensity_threshold = 3 # Threshold for filtering (in terms of RMS)\n",
    "process_audio(file_path, intensity_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensevoice_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
